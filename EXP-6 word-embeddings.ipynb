{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7LkGfi9QpjU"
   },
   "source": [
    "# Playing with embeddings \n",
    "\n",
    "OHE and Word Embeddings are the two most common text preprocessing techniques used to convert text into numbers, \n",
    "so the text sentences can be feed to Machine Learning (ML) and Deep Learning (DL) models.\n",
    "\n",
    "Intelligent models don't take as input raw text, they only work with numeric data. \n",
    "*Vectorizing* text is the process of transforming text into numeric tensors.\n",
    "\n",
    "There are different ways to vectorize raw text. The top three popular vectorizing methods are:\n",
    "\n",
    "- Segment text into words, and transform each word into a vector.\n",
    "- Segment text into characters, and transform each character into a vector.\n",
    "- Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "\n",
    " *N-grams* are overlapping groups of multiple consecutive words or characters.\n",
    "\n",
    "The different units which a sentence is split into, are called *tokens*. \n",
    "\n",
    "All *vectorizing* methods consist of first *tokenizing* the sentence (Split a sentence into different units) and then associating a vector to each *token*.\n",
    "\n",
    "*word embeddings*,  is the most used word-level *vectorization* mehtod among the DL community.\n",
    "\n",
    "#### Vectorization:\n",
    "![Vectorization](https://freecontent.manning.com/wp-content/uploads/Chollet_DLfT_01.png)\n",
    "\n",
    "\n",
    "## One-hot Encoding (OHE)\n",
    "\n",
    "OHE is the most basic way to convert a token into a vector. It consists of associating a unique integer index with every word and then turning this integer index *i* into a binary vector of size N (the size of the vocabulary); the vector is all zeros except for the *i* th entry, which is 1.\n",
    "\n",
    "We can use scikit-learn(sklearn) to convert tokens into vectors using OHE. To do so, sklearn provides a `OneHotEncoder` class, which like all the other sklearn classes, has a `fit` and `transform` method. These methods do the following tasks:\n",
    "\n",
    "- `fit`: Learns the vocabulary of our dataset, and learns how to associate a token with an integer index.\n",
    "- `transform`: Accordingly with the understanding acquired in the `fit` method transforms each token to its associated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4FdzItAjQzNw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Import ohe class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Declare text examples\n",
    "texts = ['I study Computer Science at university',\n",
    "         'My dog is called Peludet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFY_ihnakGwV"
   },
   "source": [
    "As we explained on the introduction, first we need to tokenize the texts. \n",
    "\n",
    "We will be working with a word-level tokenization, so we are going to split sentences by `' '` character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "e0WKNOD1kh-K",
    "outputId": "9f864819-0942-4542-d184-53133f43c816"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'study', 'Computer', 'Science', 'at', 'university'],\n",
       " ['My', 'dog', 'is', 'called', 'Peludet']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split texts by ' '\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "OIlFrXJhkuWg",
    "outputId": "fab8b284-defa-44f5-e51a-995a9b2bd954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I'],\n",
       " ['study'],\n",
       " ['Computer'],\n",
       " ['Science'],\n",
       " ['at'],\n",
       " ['university'],\n",
       " ['My'],\n",
       " ['dog'],\n",
       " ['is'],\n",
       " ['called'],\n",
       " ['Peludet']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flat the tokens lists so they are stored in a single list\n",
    "tokenized_texts = [[t] for tokens in tokenized_texts for t in tokens]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQrrm_zxlXvd"
   },
   "source": [
    "Now we are ready to feed the tokens to sklearn `OneHotEncoder` class `fit` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "F2_731EVlk8s",
    "outputId": "36a55b2a-3b0a-4d47-af82-d466acb35b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['x0_Computer' 'x0_I' 'x0_My' 'x0_Peludet' 'x0_Science' 'x0_at'\n",
      " 'x0_called' 'x0_dog' 'x0_is' 'x0_study' 'x0_university']\n",
      "Vocabulary len: 11\n"
     ]
    }
   ],
   "source": [
    "# Create OneHotEncoder instance\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Feed the tokens to fit method\n",
    "ohe.fit(tokenized_texts)\n",
    "\n",
    "# See what our OneHotEncoder has learned\n",
    "print('Vocabulary:', ohe.get_feature_names_out())\n",
    "print('Vocabulary len:', len(ohe.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F4fH4d-LJhH"
   },
   "source": [
    "Once we have tokenized our input sentence, it is time to transform our tokens into vectors.\n",
    "\n",
    "When working with OHE this process is quite straightforward. Although implementing OHE by hand is trivial, sklearn facilitates, even more, the tasks.\n",
    "\n",
    "We only need to call `transform` on our fitted `OneHotEncoder` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "NyQQvVRrmhlQ",
    "outputId": "8b3fac55-bd76-43ab-dafe-1b0358025729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: I -> Vector: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Token: study -> Vector: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "Token: Computer -> Vector: [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Token: Science -> Vector: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Token: at -> Vector: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Token: university -> Vector: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can covert out tokens to vectors\n",
    "## For a more clear example we will only vectorize first sentence\n",
    "encoded_sentence = ohe.transform(np.array(texts[0].split()).reshape(-1, 1))\n",
    "for t, v in zip(texts[0].split(), encoded_sentence):\n",
    "  print('Token: {} -> Vector: {}'.format(t, v.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOrBapHUot5U"
   },
   "source": [
    "As we can see each token is associated with an integer value *i*, which corresponds to the array *i*th position which at the time it is set to 1.\n",
    "\n",
    "![One Hot Encoding](https://raw.githubusercontent.com/tensorflow/docs/r2.0rc/site/en/r2/tutorials/text/images/one-hot.png)\n",
    "\n",
    "**Even though OHE is simple, it is inefficient**. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero.\n",
    "\n",
    "A part from the inefficiency, OHE has another big problem. Imagine that we are trying to vectorize these three words:\n",
    "\n",
    "- great\n",
    "- good\n",
    "- bad\n",
    "\n",
    "If we vectorize them using OHE we will get this result:\n",
    "\n",
    "```python\n",
    "assert good  == [1, 0, 0]\n",
    "assert great == [0, 1, 0]\n",
    "assert bad   == [0, 0, 1]\n",
    "```\n",
    "\n",
    "In this case, OHE vectorization result is telling us that `good` is as different as `great` and `bad` and actually this is false. So we need another vectorization method that can handle **semantic similarities**, here is where **Word Embeddings** is important.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "A more powerful way to convert tokens into vectors is the use of *dense vectors*. Those dense vectors are tokens' representations in which similar words have similar vectors. More precisely, we call those vectors *embeddings*.\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n",
    "Embeddings are more useful then OHE when our problem involves a huge vocabulary. OHE needs vectors of length N to encode words belonging to a vocabulary of N tokens. On the other hand, Word Embedding uses a fixed length to encode a vocabulary of N tokens. It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies. So, word embeddings pack more information into ar fewer dimensions.\n",
    "\n",
    "Hopefully, we don't have to think a way to fill those M-dimentional dense vectors manually. Usually embeddings are obtained via two approaches:\n",
    "\n",
    "1. Learn word embeddings joinly with the main task we care about. So in this case embeddings are an extra layer on our neural network that their weights are updated during backpropagation jointly with the task we are trying to solve, for example sentiment analysis. See [this tutorial](https://www.tensorflow.org/beta/tutorials/text/word_embeddings) to learn how to train word embeddings from scratch.\n",
    "\n",
    "2. Load a matrix containing word embedding that were precomputed using a different machine-learning task. These are called pretrained *word embeddings*.\n",
    "\n",
    "In this article we will focus on the second way of obtaining word embeddings. More precisely we are going to use GloVe embeddings [[4](#references)]. We are going to explore the embeddings learned from 6Billion tokens of text coming from Wikipedia.\n",
    "\n",
    "> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
    "\n",
    "### Playing with pretrained embeddings\n",
    "\n",
    "\n",
    "In this section, we are going to use some vector operations and properties to see how embeddings behave and thus extract some useful takeaways. We are going to find the answer to the following sentences and more:\n",
    "\n",
    "- What's up near a single embedding?\n",
    "- What happens if we add two embeddings?\n",
    "\n",
    "To start our journeym we first need to download the pretrained embeddings from the GloVe website. To get the `zip` file containing the embeddings we are going to use a shell *mini script*:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word-embeddings.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/Guillem96/playing-with-word-embeddings/blob/master/playing-word-embeddings.ipynb",
     "timestamp": 1653488182606
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
