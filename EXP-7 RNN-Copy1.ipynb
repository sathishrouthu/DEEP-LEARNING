{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on training data is 80.450550\n",
      "The accuracy on testing data is 0.231667\n",
      "The loss on training data is 79.207397\n",
      "The loss on training data is 78.016659\n",
      "The loss on training data is 76.835144\n",
      "The loss on training data is 75.631562\n",
      "The loss on training data is 74.386566\n",
      "The loss on training data is 73.082298\n",
      "The loss on training data is 71.698189\n",
      "The loss on training data is 70.211099\n",
      "The loss on training data is 68.561307\n",
      "The loss on training data is 66.697922\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 64.624710\n",
      "The loss on training data is 62.186664\n",
      "The loss on training data is 59.422074\n",
      "The loss on training data is 56.373359\n",
      "The loss on training data is 53.036751\n",
      "The loss on training data is 49.554646\n",
      "The loss on training data is 46.325710\n",
      "The loss on training data is 43.910346\n",
      "The loss on training data is 42.232201\n",
      "The loss on training data is 40.936272\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 39.824455\n",
      "The loss on training data is 38.792521\n",
      "The loss on training data is 37.812196\n",
      "The loss on training data is 36.886136\n",
      "The loss on training data is 36.038387\n",
      "The loss on training data is 35.297779\n",
      "The loss on training data is 34.672199\n",
      "The loss on training data is 34.150028\n",
      "The loss on training data is 33.712174\n",
      "The loss on training data is 33.340286\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 33.017981\n",
      "The loss on training data is 32.736931\n",
      "The loss on training data is 32.488824\n",
      "The loss on training data is 32.267263\n",
      "The loss on training data is 32.067722\n",
      "The loss on training data is 31.886589\n",
      "The loss on training data is 31.720928\n",
      "The loss on training data is 31.567823\n",
      "The loss on training data is 31.426169\n",
      "The loss on training data is 31.294822\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 31.172332\n",
      "The loss on training data is 31.053575\n",
      "The loss on training data is 30.940520\n",
      "The loss on training data is 30.834113\n",
      "The loss on training data is 30.733657\n",
      "The loss on training data is 30.638493\n",
      "The loss on training data is 30.548082\n",
      "The loss on training data is 30.462146\n",
      "The loss on training data is 30.380296\n",
      "The loss on training data is 30.302188\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 30.227523\n",
      "The loss on training data is 30.156020\n",
      "The loss on training data is 30.087453\n",
      "The loss on training data is 30.021617\n",
      "The loss on training data is 29.958311\n",
      "The loss on training data is 29.897348\n",
      "The loss on training data is 29.838598\n",
      "The loss on training data is 29.781922\n",
      "The loss on training data is 29.727186\n",
      "The loss on training data is 29.674272\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 29.623064\n",
      "The loss on training data is 29.573451\n",
      "The loss on training data is 29.525365\n",
      "The loss on training data is 29.478723\n",
      "The loss on training data is 29.433448\n",
      "The loss on training data is 29.389469\n",
      "The loss on training data is 29.346718\n",
      "The loss on training data is 29.305135\n",
      "The loss on training data is 29.264662\n",
      "The loss on training data is 29.225247\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 29.186840\n",
      "The loss on training data is 29.149396\n",
      "The loss on training data is 29.112870\n",
      "The loss on training data is 29.077226\n",
      "The loss on training data is 29.042433\n",
      "The loss on training data is 29.008444\n",
      "The loss on training data is 28.975226\n",
      "The loss on training data is 28.942748\n",
      "The loss on training data is 28.910981\n",
      "The loss on training data is 28.879896\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 28.849467\n",
      "The loss on training data is 28.819671\n",
      "The loss on training data is 28.790482\n",
      "The loss on training data is 28.761880\n",
      "The loss on training data is 28.733844\n",
      "The loss on training data is 28.706364\n",
      "The loss on training data is 28.679409\n",
      "The loss on training data is 28.652962\n",
      "The loss on training data is 28.626979\n",
      "The loss on training data is 28.601474\n",
      "The accuracy on testing data is 0.910833\n",
      "The loss on training data is 28.576421\n",
      "The loss on training data is 28.551813\n",
      "The loss on training data is 28.527634\n",
      "The loss on training data is 28.503871\n",
      "The loss on training data is 28.480506\n",
      "The loss on training data is 28.457532\n",
      "The loss on training data is 28.434938\n",
      "The loss on training data is 28.412713\n",
      "The loss on training data is 28.390846\n",
      "The loss on training data is 28.369327\n",
      "The accuracy on testing data is 0.910833\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth, lr=0.002):\n",
    "        self.lr = lr\n",
    "        self.depth = depth\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.U = xavier_init(input_dim, hidden_dim, fc=True)\n",
    "        self.W = xavier_init(hidden_dim, hidden_dim, fc=True)\n",
    "        self.V = xavier_init(hidden_dim, output_dim, fc=True)\n",
    "        # tmp variable\n",
    "        self.x = None\n",
    "        self.H = None\n",
    "        self.Alpha = None\n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        batch_size = x.shape[1]\n",
    "        self.x = x\n",
    "        self.H = np.zeros((self.depth, batch_size, self.hidden_dim))\n",
    "        self.Alpha = np.zeros((self.depth, batch_size, self.hidden_dim))\n",
    "        h_prev = np.zeros((batch_size, self.hidden_dim))\n",
    "        sigmoid_output = np.zeros((self.depth, batch_size, self.output_dim))\n",
    "        for t in range(self.depth):\n",
    "            self.Alpha[t] = self.x[t]@self.U + h_prev@self.W\n",
    "            self.H[t] = self.relu(self.Alpha[t])\n",
    "            o_t = self.H[t]@self.V\n",
    "            y_t = self.sigmoid(o_t)\n",
    "            sigmoid_output[t] = y_t\n",
    "            h_prev = self.H[t]\n",
    "        return sigmoid_output\n",
    "\n",
    "    def backward_prop(self, sigmoid_output, output_label):\n",
    "        batch_size = output_label.shape[1]\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dH_t_front = np.zeros((batch_size, self.hidden_dim))\n",
    "        for t in range(self.depth-1, 0, -1):\n",
    "            dY_t = sigmoid_output[t] - output_label[t]\n",
    "            dO_t = dY_t * sigmoid_output[t] * (1 - sigmoid_output[t])\n",
    "            dV += self.H[t].T @ dO_t\n",
    "            dH_t = dO_t @ self.V.T + dH_t_front\n",
    "            dAlpha_t = self.relu(self.Alpha[t], dH_t, deriv=True)\n",
    "            dU += self.x[t].T @ dAlpha_t\n",
    "            if t > 0:\n",
    "                dW += self.H[t-1].T @ dAlpha_t\n",
    "            dH_t_front = dAlpha_t @ self.W.T\n",
    "        self.U -= self.lr * dU\n",
    "        self.W -= self.lr * dW\n",
    "        self.V -= self.lr * dV\n",
    "\n",
    "    def relu(self, x, front_delta=None, deriv=False):\n",
    "        if deriv == False:\n",
    "            return x * (x > 0)\n",
    "        else:\n",
    "            back_delta = front_delta * 1. * (x > 0)\n",
    "            return back_delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.where(x >= 0,\n",
    "                        1 / (1 + np.exp(-x)),\n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "\n",
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "\n",
    "# The X of this dataset is @data_size number of floats lies within (0, 1)\n",
    "# The Y of this dataset is whether the current prefix sum of X has exceed the value @data_size/2.\n",
    "# X (length, data_size, 1)\n",
    "# Y (length, data_size, 1)\n",
    "def generate_dataset(data_size, length, split_ratio):\n",
    "    X = np.random.uniform(0, 1, (data_size, length, 1))\n",
    "    Y = np.zeros((data_size, length, 1))\n",
    "    threshold = length / 2.\n",
    "    for i in range(data_size):\n",
    "        prefix_sum = 0\n",
    "        for j in range(length):\n",
    "            prefix_sum += X[i][j][0]\n",
    "            Y[i][j][0] = int(prefix_sum > threshold)\n",
    "    split_point = int(data_size * split_ratio)\n",
    "    train_x, test_x = X[:split_point], X[split_point:]\n",
    "    train_y, test_y = Y[:split_point], Y[split_point:]\n",
    "    return np.swapaxes(train_x, 0, 1), np.swapaxes(test_x, 0, 1), \\\n",
    "           np.swapaxes(train_y, 0, 1), np.swapaxes(test_y, 0, 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    length = 12\n",
    "    data_size = 1000\n",
    "    split_ratio = 0.9\n",
    "    max_iter = 100\n",
    "    iters_before_test = 10\n",
    "    batch_size = 25\n",
    "    train_x, test_x, train_y, test_y = generate_dataset(data_size, length, split_ratio)\n",
    "    rnn = RNN(1, 10, 1, length)\n",
    "    for iters in range(max_iter+1):\n",
    "        st_idx = int(iters % ((split_ratio * length) / batch_size))\n",
    "        ed_idx = int(st_idx + batch_size)\n",
    "        sigmoid_output = rnn.forward_prop(train_x[:, st_idx:ed_idx, :])\n",
    "        rnn.backward_prop(sigmoid_output, train_y[:, st_idx:ed_idx, :])\n",
    "        loss = np.sum((sigmoid_output - train_y[:, st_idx:ed_idx, :]) ** 2)\n",
    "        print(\"The loss on training data is %f\" % loss)\n",
    "        if iters % iters_before_test == 0:\n",
    "            sigmoid_output = rnn.forward_prop(test_x)\n",
    "            predict_label = sigmoid_output > 0.5\n",
    "            accuracy = float(np.sum(predict_label == test_y.astype(bool))) / test_y.size\n",
    "            print(\"The accuracy on testing data is %f\" % accuracy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Site1997/RNN-implementation/blob/master/rnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
